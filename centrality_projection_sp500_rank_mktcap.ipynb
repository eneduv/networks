{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine(\"postgresql+psycopg2://db_reader:dbreader@localhost:5432/factset\")\n",
    "sp_engine = create_engine(\"postgresql+psycopg2://db_reader:dbreader@localhost:5432/portfolio\")\n",
    "sp500_cst = pd.read_sql(\"\"\"SELECT c.gvkey, n.tic, c.from, c.thru, c.iid FROM comp_idxcst_his c\n",
    "                                    LEFT JOIN comp_namesd n ON c.gvkey= n.gvkey AND c.iid = n.iid\n",
    "                                    WHERE c.gvkeyx='000003'\n",
    "                                    AND c.from<= '2018-09-03'\n",
    "                                    AND (c.thru IS NULL OR c.thru>='2018-09-03')\"\"\", sp_engine)\n",
    "tickers = pd.read_sql(\"SELECT t.ticker_region, e.factset_entity_id FROM fds.sym_v1_sym_ticker_region t LEFT JOIN fds.sym_v1_sym_coverage s ON t.fsym_id=s.fsym_id LEFT JOIN fds.ent_v1_ent_scr_sec_entity e ON s.fsym_security_id=e.fsym_id;\", engine)\n",
    "tickers['ticker'], tickers['region'] = tickers['ticker_region'].str.split('-', 1).str\n",
    "sp500_entity_id = pd.merge(sp500_cst, tickers.dropna(), left_on='tic', right_on='ticker', how='left')\n",
    "sp500_entity_id = sp500_entity_id[sp500_entity_id['region']=='US'].sort_values('tic').drop_duplicates(['tic']).reset_index(drop=True)\n",
    "sp500_entity_id = sp500_entity_id[~sp500_entity_id['ticker'].isin(['DISCK','FOX','GOOG','NWS','UA'])]\n",
    "sp500_ticker_entity_id = sp500_entity_id[['ticker','factset_entity_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_entity_ids = list(sp500_entity_id['factset_entity_id'])\n",
    "sp500_entity_id.to_csv('/mnt/hdd/data/SP500_factset_entity_id_to_ticker.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supl_chain = pd.read_sql(\"SELECT * from fds.ent_v1_ent_scr_relevance_rank;\", engine)\n",
    "overlap = pd.read_sql(\"SELECT source_factset_entity_id, target_factset_entity_id, start_date, end_date, overlap, direct_particip_overlap from fds.ent_v1_ent_scr_relationships_summary;\", engine)\n",
    "neighbor_supl_chain = supl_chain[(supl_chain['supplier_factset_entity_id'].isin(sp500_entity_ids)) | (supl_chain['customer_factset_entity_id'].isin(sp500_entity_ids))]\n",
    "neighbor_supl_chain.replace('003JLG-E','0FPWZZ-E')\n",
    "SP500_comp = overlap[(overlap['source_factset_entity_id'].isin(sp500_entity_ids)) & (overlap['target_factset_entity_id'].isin(sp500_entity_ids))]\n",
    "neighbor_supl_chain.to_csv('/mnt/hdd/data/SP500_neighbor_supl_chain.csv', index=False)\n",
    "SP500_comp.to_csv('/mnt/hdd/data/SP500_comp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_entity_id['gvkey-iid'] = sp500_entity_id['gvkey'] + '-' + sp500_entity_id['iid'].str.strip()\n",
    "sp500_entity_id_gvkey = dict(zip(sp500_entity_id['gvkey-iid'], sp500_entity_id['factset_entity_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_supl_chain.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(tickers[['factset_entity_id','ticker']], neighbor_supl_chain, left_on='factset_entity_id', right_on='supplier_factset_entity_id', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplier_revenue_available_count = pd.DataFrame(neighbor_supl_chain.groupby('supplier_factset_entity_id')[['customer_revenue_pct']].count().reindex(sp500_entity_ids)).rename(columns={'customer_revenue_pct':'supplier_revenue_available_count'}).merge(node_df[['vertex_id','entity_proper_name','factset_sector_desc','factset_industry_desc']], left_index=True, right_on='vertex_id').set_index('vertex_id')\n",
    "supplier_revenue_count = pd.DataFrame(neighbor_supl_chain.groupby('supplier_factset_entity_id')[['customer_revenue_pct']].size().reindex(sp500_entity_ids)).rename(columns={0:'supplier_revenue_count'}).merge(node_df[['vertex_id','entity_proper_name','factset_sector_desc','factset_industry_desc']], left_index=True, right_on='vertex_id').set_index('vertex_id')\n",
    "supplier_revenue_available_pct = pd.DataFrame(supplier_revenue_available_count['supplier_revenue_available_count']/supplier_revenue_count['supplier_revenue_count']).rename(columns={'supplier_revenue_available_count':'supplier_revenue_available_pct'}).merge(node_df[['vertex_id','entity_proper_name','factset_sector_desc','factset_industry_desc']], left_index=True, right_on='vertex_id').set_index('vertex_id')\n",
    "customer_revenue_available_count = pd.DataFrame(neighbor_supl_chain.groupby('customer_factset_entity_id')[['customer_revenue_pct']].count().reindex(sp500_entity_ids)).rename(columns={'customer_revenue_pct':'customer_revenue_available_count'}).merge(node_df[['vertex_id','entity_proper_name','factset_sector_desc','factset_industry_desc']], left_index=True, right_on='vertex_id').set_index('vertex_id')\n",
    "customer_revenue_count = pd.DataFrame(neighbor_supl_chain.groupby('customer_factset_entity_id')[['customer_revenue_pct']].size().reindex(sp500_entity_ids)).rename(columns={0:'customer_revenue_count'}).merge(node_df[['vertex_id','entity_proper_name','factset_sector_desc','factset_industry_desc']], left_index=True, right_on='vertex_id').set_index('vertex_id')\n",
    "customer_revenue_available_pct = pd.DataFrame(customer_revenue_available_count['customer_revenue_available_count']/customer_revenue_count['customer_revenue_count']).rename(columns={'customer_revenue_available_count':'customer_revenue_available_pct'}).merge(node_df[['vertex_id','entity_proper_name','factset_sector_desc','factset_industry_desc']], left_index=True, right_on='vertex_id').set_index('vertex_id')\n",
    "\n",
    "supplier_revenue_available_count.to_csv('/mnt/hdd/results/centrality/supplier_revenue_available_count.csv')\n",
    "supplier_revenue_count.to_csv('/mnt/hdd/results/centrality/supplier_revenue_count.csv')\n",
    "supplier_revenue_available_pct.to_csv('/mnt/hdd/results/centrality/supplier_revenue_available_pct.csv')\n",
    "customer_revenue_available_count.to_csv('/mnt/hdd/results/centrality/customer_revenue_available_count.csv')\n",
    "customer_revenue_count.to_csv('/mnt/hdd/results/centrality/customer_revenue_count.csv')\n",
    "customer_revenue_available_pct.to_csv('/mnt/hdd/results/centrality/customer_revenue_available_pct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.to_datetime(\"2018-01-01\",format=\"%Y-%m-%d\")\n",
    "current_neighbors_supl_chain = neighbors_supl_chain[(neighbors_supl_chain['start_date']<=d) & ((neighbors_supl_chain['end_date']>d) | neighbors_supl_chain['end_date'].isnull())]\n",
    "supplier_revenue_available_count = pd.DataFrame(current_neighbors_supl_chain.groupby('supplier_factset_entity_id')[['customer_revenue_pct']].count().reindex(sp500_entity_ids)).rename(columns={'customer_revenue_pct':'supplier_revenue_available_count'}).merge(node_df[['vertex_id','entity_proper_name','factset_sector_desc','factset_industry_desc']], left_index=True, right_on='vertex_id').set_index('vertex_id')\n",
    "supplier_revenue_count = pd.DataFrame(current_neighbors_supl_chain.groupby('supplier_factset_entity_id')[['customer_revenue_pct']].size().reindex(sp500_entity_ids)).rename(columns={0:'supplier_revenue_count'}).merge(node_df[['vertex_id','entity_proper_name','factset_sector_desc','factset_industry_desc']], left_index=True, right_on='vertex_id').set_index('vertex_id')\n",
    "supplier_revenue_available_pct = pd.DataFrame(supplier_revenue_available_count['supplier_revenue_available_count']/supplier_revenue_count['supplier_revenue_count']).rename(columns={'supplier_revenue_available_count':'supplier_revenue_available_pct'}).merge(node_df[['vertex_id','entity_proper_name','factset_sector_desc','factset_industry_desc']], left_index=True, right_on='vertex_id').set_index('vertex_id')\n",
    "customer_revenue_available_count = pd.DataFrame(current_neighbors_supl_chain.groupby('customer_factset_entity_id')[['customer_revenue_pct']].count().reindex(sp500_entity_ids)).rename(columns={'customer_revenue_pct':'customer_revenue_available_count'}).merge(node_df[['vertex_id','entity_proper_name','factset_sector_desc','factset_industry_desc']], left_index=True, right_on='vertex_id').set_index('vertex_id')\n",
    "customer_revenue_count = pd.DataFrame(current_neighbors_supl_chain.groupby('customer_factset_entity_id')[['customer_revenue_pct']].size().reindex(sp500_entity_ids)).rename(columns={0:'customer_revenue_count'}).merge(node_df[['vertex_id','entity_proper_name','factset_sector_desc','factset_industry_desc']], left_index=True, right_on='vertex_id').set_index('vertex_id')\n",
    "customer_revenue_available_pct = pd.DataFrame(customer_revenue_available_count['customer_revenue_available_count']/customer_revenue_count['customer_revenue_count']).rename(columns={'customer_revenue_available_count':'customer_revenue_available_pct'}).merge(node_df[['vertex_id','entity_proper_name','factset_sector_desc','factset_industry_desc']], left_index=True, right_on='vertex_id').set_index('vertex_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.to_datetime(\"2018-01-01\",format=\"%Y-%m-%d\")\n",
    "current_neighbors_supl_chain = neighbors_supl_chain[(neighbors_supl_chain['start_date']<=d) & ((neighbors_supl_chain['end_date']>d) | neighbors_supl_chain['end_date'].isnull())]\n",
    "supplier_revenue_available_count = pd.DataFrame(current_neighbors_supl_chain.groupby('supplier_factset_entity_id')[['customer_revenue_pct']].count().reindex(sp500_entity_ids)).rename(columns={'customer_revenue_pct':'supplier_revenue_available_count'}).merge(node_df[['vertex_id','entity_proper_name','factset_sector_desc','factset_industry_desc']], left_index=True, right_on='vertex_id').set_index('vertex_id')\n",
    "supplier_revenue_count = pd.DataFrame(current_neighbors_supl_chain.groupby('supplier_factset_entity_id')[['customer_revenue_pct']].size().reindex(sp500_entity_ids)).rename(columns={0:'supplier_revenue_count'}).merge(node_df[['vertex_id','entity_proper_name','factset_sector_desc','factset_industry_desc']], left_index=True, right_on='vertex_id').set_index('vertex_id')\n",
    "supplier_revenue_available_pct = pd.DataFrame(supplier_revenue_available_count['supplier_revenue_available_count']/supplier_revenue_count['supplier_revenue_count']).rename(columns={'supplier_revenue_available_count':'supplier_revenue_available_pct'}).merge(node_df[['vertex_id','entity_proper_name','factset_sector_desc','factset_industry_desc']], left_index=True, right_on='vertex_id').set_index('vertex_id')\n",
    "customer_revenue_available_count = pd.DataFrame(current_neighbors_supl_chain.groupby('customer_factset_entity_id')[['customer_revenue_pct']].count().reindex(sp500_entity_ids)).rename(columns={'customer_revenue_pct':'customer_revenue_available_count'}).merge(node_df[['vertex_id','entity_proper_name','factset_sector_desc','factset_industry_desc']], left_index=True, right_on='vertex_id').set_index('vertex_id')\n",
    "customer_revenue_count = pd.DataFrame(current_neighbors_supl_chain.groupby('customer_factset_entity_id')[['customer_revenue_pct']].size().reindex(sp500_entity_ids)).rename(columns={0:'customer_revenue_count'}).merge(node_df[['vertex_id','entity_proper_name','factset_sector_desc','factset_industry_desc']], left_index=True, right_on='vertex_id').set_index('vertex_id')\n",
    "customer_revenue_available_pct = pd.DataFrame(customer_revenue_available_count['customer_revenue_available_count']/customer_revenue_count['customer_revenue_count']).rename(columns={'customer_revenue_available_count':'customer_revenue_available_pct'}).merge(node_df[['vertex_id','entity_proper_name','factset_sector_desc','factset_industry_desc']], left_index=True, right_on='vertex_id').set_index('vertex_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplier_revenue_available_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_revenue_available_count['customer_revenue_available_count'].sum()/customer_revenue_count['customer_revenue_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplier_revenue_available_count['supplier_revenue_available_count'].sum()/supplier_revenue_count['supplier_revenue_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT c.datadate, c.gvkey, c.iid, c.cshoc, c.cshtrd, c.prccd, c.trfd, c.ajexdi FROM \n",
    "            comp_secd c INNER JOIN (SELECT DISTINCT gvkey, iid FROM comp_idxcst_his WHERE gvkeyx='000003' AND \n",
    "            \"from\"<= '2018-09-03' AND (thru IS NULL OR thru>='2018-09-03')) AS h ON c.gvkey=h.gvkey AND c.iid = h.iid LEFT JOIN comp_namesd n ON c.gvkey= n.gvkey AND \n",
    "            c.iid = n.iid WHERE c.datadate BETWEEN '1980-01-01' AND '2018-12-30'; \"\"\"\n",
    "data = pd.read_sql(query, sp_engine)\n",
    "data['gvkey-iid'] = data['gvkey'] + '-' + data['iid'].str.strip()\n",
    "data['trfd'].fillna(1.0, inplace=True)\n",
    "data['ajexdi'].fillna(1.0, inplace=True)\n",
    "data['adj_close'] = data['prccd'] * data['trfd'] / data['ajexdi']\n",
    "data['datadate'] = pd.to_datetime(data['datadate'])\n",
    "data = data.dropna(subset=['cshtrd', 'prccd'])\n",
    "price = data.pivot(index='datadate', columns='gvkey-iid',\n",
    "                   values=['adj_close', 'cshtrd', 'cshoc']).sort_index().dropna(\n",
    "    axis=0, how='all')\n",
    "price = price['adj_close'].rename(columns=sp500_entity_id_gvkey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "alldates = price.index\n",
    "start_date = '1984-01-01'\n",
    "end_date = '2018-12-31'\n",
    "dates = []\n",
    "end_date = pd.to_datetime(end_date, format=\"%Y-%m-%d\")\n",
    "start_date = pd.to_datetime(start_date, format=\"%Y-%m-%d\")\n",
    "current_date = start_date\n",
    "while current_date < end_date:\n",
    "    dates.append(current_date)\n",
    "    current_date += relativedelta(months=3)\n",
    "    \n",
    "sorted_dates = sorted([next(v for i, v in enumerate(alldates) if v >= d) for d in dates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = (np.log(price.loc[sorted_dates]) - np.log(price.loc[sorted_dates].shift(1)))[1:].sort_index().dropna(axis=0, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import progressbar\n",
    "import igraph as ig\n",
    "\n",
    "\n",
    "def corr_matrix(ret, thresh=0.95, window=250, enddate=\"2017-01-24\", shrinkage=None, index_ret=None, exp_shrinkage_theta=125,detrended=False):\n",
    "    \"\"\"Generates correlation matrix for a window that ends on enddate. Correlation can have exponential shrinkage (giving more weights to recent observations.)\n",
    "    index_ret is used for detrending. If None, will use average return of all assets.\n",
    "    Will only use assets with more than thresh%% data available in the window\"\"\"\n",
    "    end = list(ret.index).index(enddate) + 1\n",
    "    start = end - window\n",
    "    subret = ret.values[start:end]\n",
    "    if not (index_ret is None):\n",
    "        end = list(index_ret.index).index(enddate) + 1\n",
    "        start = end - window\n",
    "        index_subret = index_ret.values[start:end].flatten()\n",
    "    eligible = (~np.isnan(subret)).sum(axis=0) >= thresh * window\n",
    "    subret = subret[:, eligible]\n",
    "    company_names = ret.columns[eligible]\n",
    "    # drop whole column when there are less than or equal to\n",
    "    # thresh number of non-nan entries in the window\n",
    "    # sub = ret[start:end]\n",
    "    subret[np.isnan(subret)] = 0\n",
    "    if detrended:\n",
    "        r = subret\n",
    "        if not (index_ret is None):\n",
    "            I = index_subret\n",
    "        else:\n",
    "            I = subret.mean(axis=1)\n",
    "        n = len(I)\n",
    "        alpha = (r.sum(axis=0) * (I * I).sum() - I.sum() * r.T.dot(I)) / (n * (I * I).sum() - (I.sum()) ** 2)\n",
    "        beta = (n * r.T.dot(I) - I.sum() * r.sum(axis=0)) / (n * (I * I).sum() - (I.sum()) ** 2)\n",
    "        c = r - alpha - np.outer(I, beta)\n",
    "        # temp = pd.DataFrame(c)\n",
    "        # temp.index = company_names\n",
    "        # temp.columns = company_names\n",
    "        subret = c\n",
    "    if shrinkage is None:\n",
    "        corr_mat = pd.DataFrame(np.corrcoef(subret, rowvar=False))\n",
    "        corr_mat.columns = company_names\n",
    "        corr_mat.index = company_names\n",
    "    # elif shrinkage == \"LedoitWolf\":\n",
    "    #     cov = ledoit_wolf(subret, assume_centered=True)[0]\n",
    "    #     std = np.sqrt(np.diagonal(cov))\n",
    "    #     corr_mat = (cov / std[:, None]).T / std[:, None]\n",
    "    #     np.fill_diagonal(corr_mat, 1.0)\n",
    "    #     corr_mat = pd.DataFrame(data=corr_mat, index=subret.columns, columns=subret.columns)\n",
    "    elif shrinkage == \"Exponential\":\n",
    "        stocknames = company_names\n",
    "        weight_list = np.exp((np.arange(1, window + 1) - window) / exp_shrinkage_theta)\n",
    "        weight_list = weight_list / weight_list.sum()\n",
    "        cov = np.cov(subret, rowvar=False, aweights=weight_list)\n",
    "        cov_diag = np.sqrt(np.diag(cov))\n",
    "        corr = (cov / cov_diag).T / cov_diag\n",
    "        corr_mat = pd.DataFrame(corr)\n",
    "        corr_mat.columns = stocknames\n",
    "        corr_mat.index = stocknames\n",
    "    else:\n",
    "        print(\"'shrinkage' can only be None or 'Exponential'\")\n",
    "        return None\n",
    "    # corr_mat.apply(lambda x:1-x**2 if not math.isnan(x) else np.nan)\n",
    "    return corr_mat\n",
    "\n",
    "def all_corr(ret, thresh=0.95, inclusion=pd.DataFrame(), window=250, shrinkage=None,exp_shrinkage_theta=125, detrended=False, store=None):\n",
    "    \"\"\"Computes correlations on all dates in the ret dataframe\"\"\"\n",
    "    print(\"Computing all correlations with window=%s, shrinkage=%s, theta=%s...\" % (window, shrinkage, exp_shrinkage_theta))\n",
    "    if store is None:\n",
    "        allcorr = {}\n",
    "    else:\n",
    "        allcorr = store\n",
    "    alldates = ret.index\n",
    "    alldates.sort_values()\n",
    "    bar = progressbar.ProgressBar(max_value=len(alldates[window:]))\n",
    "    for d in alldates[window:]:\n",
    "        if inclusion.empty:\n",
    "            allcorr[str(d.strftime(\"%Y-%m-%d\"))] = corr_matrix(ret, thresh, window, enddate=d, shrinkage=shrinkage, exp_shrinkage_theta=exp_shrinkage_theta,detrended=detrended)\n",
    "        else:\n",
    "            eligible_stocks = list(inclusion[(inclusion['from']<=d) & ((inclusion['thru'].isnull()) | (inclusion['thru']>=d))]['PERMNO'].unique())\n",
    "            allcorr[str(d.strftime(\"%Y-%m-%d\"))] = \\\n",
    "            corr_matrix(ret[eligible_stocks], thresh, window, enddate=d, shrinkage=shrinkage, exp_shrinkage_theta = exp_shrinkage_theta, detrended=detrended)\n",
    "        bar+=1\n",
    "    alldates = np.array(sorted([s[-10:] for s in allcorr.keys()]))\n",
    "    return allcorr\n",
    "\n",
    "def rolling_corr(allcorr, dates, alldates=None, average=False, tau=125, store=None):\n",
    "    \"\"\"Return a dictionary of correlation matrices.\n",
    "    The key is the enddate of the window, the value is corresponding correlation matrix\n",
    "    NA will be dropped.\"\"\"\n",
    "    if store is None:\n",
    "        result = {}\n",
    "    else:\n",
    "        result = store\n",
    "    if alldates is None:\n",
    "        alldates = np.array(sorted([s[-10:] for s in allcorr.keys()]))\n",
    "    if average==False:\n",
    "        print(\"Computing corrs without average...\")\n",
    "        bar = progressbar.ProgressBar(max_value=len(dates))\n",
    "        for d in dates:\n",
    "            d = pd.to_datetime(d).strftime(\"%Y-%m-%d\")\n",
    "            result[str(d)] = allcorr[str(d)].dropna(how='all', axis=0).dropna(how='all', axis=1)\n",
    "            bar+=1\n",
    "    else:\n",
    "        print(\"Computing adjusted_R\")\n",
    "        adjusted_R = {}\n",
    "        bar = progressbar.ProgressBar(max_value=len(alldates))\n",
    "        for d in alldates:\n",
    "            corr = allcorr[str(d)].values\n",
    "            stocknames = allcorr[str(d)].index.values\n",
    "            target = (corr.sum() - np.diag(corr).sum()) / (corr.shape[0] * (corr.shape[0] - 1))\n",
    "            temp= pd.DataFrame(corr+target)\n",
    "            temp.columns = stocknames\n",
    "            temp.index = stocknames\n",
    "            adjusted_R[str(d)] = temp\n",
    "            bar+=1\n",
    "        print(\"Computing corr with average...\")\n",
    "        bar = progressbar.ProgressBar(max_value=len(dates))\n",
    "        for d in dates:\n",
    "            d = d.strftime(\"%Y-%m-%d\")\n",
    "            windowend = np.where(alldates==d)[0]\n",
    "            if len(windowend)>0:\n",
    "                windowend = int(windowend[0])+1\n",
    "            else:\n",
    "                windowend = 0\n",
    "            windowstart = windowend-tau\n",
    "            if windowstart >= 0:\n",
    "                shrinkage_corr = sum(adjusted_R[str(dd)] for dd in alldates[windowstart:windowend])/(2*(tau+1))\n",
    "                result[str(d)] = shrinkage_corr.dropna(how='all', axis=0).dropna(how='all', axis=1)\n",
    "            bar+=1\n",
    "    return result\n",
    "\n",
    "def build_graph(corr, method='gower'):\n",
    "    \"\"\"Builds igraph graph from correlation matrix.\"\"\"\n",
    "    if method == \"gower\":\n",
    "        def distance(weight):\n",
    "            return (2 - 2 * weight) ** 0.5  # gower\n",
    "    elif method == \"power\":\n",
    "        def distance(weight):\n",
    "            return 1 - weight ** 2  # power\n",
    "    node_names = corr.columns.values\n",
    "    g = ig.Graph.Weighted_Adjacency(corr.values.tolist(), mode=\"UNDIRECTED\", attr=\"weight\", loops=False)\n",
    "    g.vs['name'] = node_names\n",
    "    g.es['weight+1'] = np.array(g.es['weight']) + 1.0\n",
    "    g.es['length'] = distance(np.array(g.es['weight']))\n",
    "    g.es['absweight'] = np.abs(np.array(g.es['weight']))\n",
    "    return g\n",
    "\n",
    "def MST(corrs, method=\"gower\"):\n",
    "    \"\"\"Returns a dictionary of Minimum Spanning Tree for each end date and their graphs in a separate dict\"\"\"\n",
    "    trees = {}\n",
    "    graphs = {}\n",
    "    print(\"Creating MSTs...\")\n",
    "    for d in corrs.keys():\n",
    "        G = build_graph(corrs[d], method)\n",
    "        graphs[d[-10:]] = G\n",
    "        T = G.spanning_tree(return_tree=True, weights='length')\n",
    "        trees[d[-10:]] = T\n",
    "    return trees, graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allcorr = all_corr(ret, thresh=0.95, shrinkage='Exponential', detrended=True, window=28, exp_shrinkage_theta=14)\n",
    "dates = [d for d in sorted_dates if d >= pd.to_datetime('2003-01-01',format=\"%Y-%m-%d\")]\n",
    "spaced_corr_store = pd.HDFStore('/mnt/hdd/results/SP500_quarterly_return_spaced_corr_test.h5')\n",
    "spaced_corr = rolling_corr(allcorr, dates, average=True, tau=14, store=spaced_corr_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_sector_info = pd.read_sql(\"\"\"SELECT e.entity_proper_name, sm.factset_sector_desc, im.factset_industry_desc, sic.sic_desc, s.* FROM fds.sym_v1_sym_entity e\n",
    "LEFT JOIN fds.sym_v1_sym_entity_sector s ON e.factset_entity_id=s.factset_entity_id \n",
    "LEFT JOIN fds.ref_v2_factset_sector_map sm ON s.sector_code=sm.factset_sector_code\n",
    "LEFT JOIN fds.ref_v2_factset_industry_map im ON s.industry_code=im.factset_industry_code\n",
    "LEFT JOIN fds.ref_v2_sic_map sic ON s.primary_sic_code=sic.sic_code\n",
    "WHERE e.factset_entity_id IN ('\"\"\"+\"', '\".join(sp500_entity_ids)+\"');\", engine)\n",
    "entity_sector_info = pd.merge(entity_sector_info, sp500_entity_id[['factset_entity_id','ticker']], how='left', left_on='factset_entity_id', right_on='factset_entity_id').rename(columns={'factset_entity_id':'vertex_id'})\n",
    "entity_sector_info.to_csv(\"/mnt/hdd/data/SP500_entity_info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "# Experiment begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_supl_chain = pd.read_csv('/mnt/hdd/data/SP500_neighbor_supl_chain.csv')\n",
    "SP500_comp = pd.read_csv('/mnt/hdd/data/SP500_comp.csv')\n",
    "neighbors_supl_chain['start_date'] = pd.to_datetime(neighbors_supl_chain['start_date'],format=\"%Y-%m-%d\")\n",
    "neighbors_supl_chain['end_date'] = pd.to_datetime(neighbors_supl_chain['end_date'],format=\"%Y-%m-%d\")\n",
    "SP500_comp['start_date'] = pd.to_datetime(SP500_comp['start_date'],format=\"%Y-%m-%d\")\n",
    "SP500_comp['end_date'] = pd.to_datetime(SP500_comp['end_date'],format=\"%Y-%m-%d\")\n",
    "node_df = pd.read_csv(\"/mnt/hdd/data/SP500_entity_info.csv\")\n",
    "corr = pd.HDFStore('/mnt/hdd/results/SP500_quarterly_return_spaced_corr_test.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.show(node_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rel_graph(data, weight=False, directed=True):\n",
    "    company_ids = sorted(list(set(data['from_id']).union(set(data['to_id']))))\n",
    "    company_dict = dict(zip(company_ids,range(len(company_ids))))\n",
    "    rel_rows = data.iterrows()\n",
    "    edge_list = [(company_dict[row['from_id']],company_dict[row['to_id']]) for _, row in rel_rows]\n",
    "    rel_g = ig.Graph(edge_list, directed=directed)\n",
    "    rel_g.vs[\"name\"] = company_ids\n",
    "    if weight:\n",
    "        edge_weight = data[weight].values\n",
    "        rel_g.es['weight'] = edge_weight\n",
    "    return rel_g\n",
    "    \n",
    "def build_corr_tree(corr_matrix, method='Gower'):\n",
    "    company_ids = corr_matrix.index.values\n",
    "    corr_G = ig.Graph.Weighted_Adjacency(corr_matrix.values.tolist(), mode='UNDIRECTED', attr=\"weight\", loops=False)\n",
    "    corr_G.vs['name'] = company_ids\n",
    "    if method == 'Gower':\n",
    "        corr_G.es['length'] = np.sqrt(2-2*np.array(corr_G.es['weight']))\n",
    "    elif method =='power':\n",
    "        corr_G.es['length'] = 1 - np.array(corr_G.es['weight'])**2\n",
    "    corr_G.es['weight+1'] = 1+np.array(corr_G.es['weight'])\n",
    "    return corr_G.spanning_tree(weights='length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection(current_mutual_df, index_cst_list, mutual_supplier=True):\n",
    "    if mutual_supplier:\n",
    "        mutual_column = 'from_id'\n",
    "        index_cst_column = 'to_id'\n",
    "    else:\n",
    "        mutual_column = 'to_id'\n",
    "        index_cst_column = 'from_id'\n",
    "    current_mutual_df = current_mutual_df.set_index(index_cst_column).sort_index()\n",
    "    tmp_arr = []\n",
    "    max_idx = len(index_cst_list)\n",
    "    for i in range(max_idx-1):\n",
    "        for j in range(i+1, max_idx):\n",
    "            source = index_cst_list[i]\n",
    "            target = index_cst_list[j]\n",
    "            try:\n",
    "                weight = len(set(current_mutual_df.at[source,mutual_column]).intersection(set(current_mutual_df.at[target,mutual_column])))\n",
    "                if weight > 0:\n",
    "                    tmp_arr.append([source,target,weight])\n",
    "            except KeyError:\n",
    "                pass\n",
    "    return pd.DataFrame(tmp_arr,columns = ['from_id','to_id','weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "def rank_mktcap_projection(current_mutual_df, index_cst_list, mutual_supplier=True):\n",
    "    if mutual_supplier:\n",
    "        mutual_column = 'from_id'\n",
    "        index_cst_column = 'to_id'\n",
    "    else:\n",
    "        mutual_column = 'to_id'\n",
    "        index_cst_column = 'from_id'\n",
    "    current_mutual_df = current_mutual_df.set_index(index_cst_column).sort_index()\n",
    "    tmp_arr = []\n",
    "    max_idx = len(index_cst_list)\n",
    "    for i in range(max_idx-1):\n",
    "        for j in range(i+1, max_idx):\n",
    "            source = index_cst_list[i]\n",
    "            target = index_cst_list[j]\n",
    "            try:\n",
    "                weight = len(set(current_mutual_df.at[source,mutual_column]).intersection(set(current_mutual_df.at[target,mutual_column])))\n",
    "                if weight > 0:\n",
    "                    tmp_arr.append([source,target,weight])\n",
    "            except KeyError:\n",
    "                pass\n",
    "    return pd.DataFrame(tmp_arr,columns = ['from_id','to_id','weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "table = pd.DataFrame()\n",
    "weighted_degree_table = pd.DataFrame()\n",
    "edges_df = pd.DataFrame()\n",
    "new_edges_df = pd.DataFrame()\n",
    "dates = sorted([key[-10:] for key in spaced_corr_store.keys()])[2:]\n",
    "for d in dates:\n",
    "    print d\n",
    "    d = pd.to_datetime(d,format=\"%Y-%m-%d\")\n",
    "    current_neighbors_supl_chain = neighbors_supl_chain[(neighbors_supl_chain['start_date']<=d) & ((neighbors_supl_chain['end_date']>d) | neighbors_supl_chain['end_date'].isnull())]\n",
    "    current_comp = SP500_comp[(SP500_comp['start_date']<=d) & ((SP500_comp['end_date']>d) | SP500_comp['end_date'].isnull())]\n",
    "    \n",
    "    \n",
    "    # ---------------- Competition Layer -----------------\n",
    "    current_comp_df = SP500_comp[(SP500_comp['start_date']<=d) & ((SP500_comp['end_date']>d) | SP500_comp['end_date'].isnull()) & (SP500_comp['direct_particip_overlap']>0)]\n",
    "    current_comp_df = current_comp_df[['source_factset_entity_id','target_factset_entity_id','direct_particip_overlap']].rename(columns={'source_factset_entity_id':'from_id', 'target_factset_entity_id':'to_id','direct_particip_overlap':\"weight\"})\n",
    "    current_comp_df = current_comp_df.set_index(['from_id','to_id'])\n",
    "    current_comp_df.index = pd.MultiIndex.from_tuples([tuple(sorted(i)) for i in current_comp_df.index.values])\n",
    "    current_comp_df = current_comp_df[~current_comp_df.index.duplicated(keep='first')].reset_index().rename(columns={'level_0':'from_id','level_1':'to_id'})\n",
    "    current_comp_df.loc[:,'rel_type'] = 'OVERLAP'\n",
    "    current_comp_df.loc[:,'net_id'] = d\n",
    "    edges_df = edges_df.append(current_comp_df)\n",
    "    \n",
    "    # ------------- Within SP100 Supply chain layer\n",
    "    # if layer in [\"Supply\", \"Multiplex\"]:\n",
    "    #     current_direct_supplier_df = current_neighbors_supl_chain[((current_neighbors_supl_chain['supplier_factset_entity_id'].isin(sp100_tickers))\n",
    "    #                                     & (current_neighbors_supl_chain['customer_factset_entity_id'].isin(sp100_tickers)))]\n",
    "    #     current_direct_supplier_df = current_direct_supplier_df[['supplier_factset_entity_id','customer_factset_entity_id','grade','ranked_factset_entity_id']].rename(columns={'supplier_factset_entity_id':'from_id', 'customer_factset_entity_id':'to_id',\"grade\":\"weight\"})\n",
    "    #     current_direct_supplier_df = current_direct_supplier_df.groupby(['from_id','to_id'])[['weight']].mean().reset_index().fillna(0)\n",
    "    #     current_direct_supplier_df.loc[:,'rel_type'] = 'DIRECT_SUPPLIER'\n",
    "    #     current_direct_supplier_df.loc[:,'net_id'] = d\n",
    "    #     edges_df = edges_df.append(current_direct_supplier_df)\n",
    "\n",
    "    # ------------ Correlation layer --------------\n",
    "    corr_matrix = corr[d.strftime(\"%Y-%m-%d\")]\n",
    "    corr_T = build_corr_tree(corr_matrix)\n",
    "    corr_T_edge_list = []\n",
    "    for e in list(corr_T.es):\n",
    "        corr_T_edge_list.append([corr_T.vs[e.source]['name'], corr_T.vs[e.target]['name'], e['weight'], 'CORRELATION', d])\n",
    "    corr_T_edge_list = pd.DataFrame(corr_T_edge_list)\n",
    "    corr_T_edge_list.columns = ['from_id','to_id','corr','rel_type','net_id']\n",
    "    # tmp_vol = pd.DataFrame(volatility.loc[d, corr_matrix.columns]).rename(columns={d:0})\n",
    "    # corr_T_edge_list = pd.merge(corr_T_edge_list,tmp_vol,\n",
    "    # left_on='from_id', right_index=True, how='left').rename(columns={0:'from_vol'})\n",
    "    # corr_T_edge_list = pd.merge(corr_T_edge_list,tmp_vol,\n",
    "    # left_on='to_id', right_index=True, how='left').rename(columns={0:'to_vol'})\n",
    "    # corr_T_edge_list['weight'] = corr_T_edge_list['corr'] * corr_T_edge_list[['from_vol','to_vol']].sum(axis=1)\n",
    "    corr_T_edge_list['weight'] = corr_T_edge_list['corr']\n",
    "    edges_df = edges_df.append(corr_T_edge_list[['from_id','to_id','weight','rel_type','net_id']])\n",
    "    \n",
    "    # -------------- Mutual supplier layer ----------------\n",
    "    current_mutual_supplier_df = current_neighbors_supl_chain[~current_neighbors_supl_chain['supplier_factset_entity_id'].isin(sp500_entity_ids)]\n",
    "    current_mutual_supplier_df = current_mutual_supplier_df.groupby('supplier_factset_entity_id').filter(lambda x: x['customer_factset_entity_id'].count() > 2).sort_values('supplier_factset_entity_id')\n",
    "    current_mutual_supplier_df = current_mutual_supplier_df[['supplier_factset_entity_id','customer_factset_entity_id','grade','ranked_factset_entity_id']].rename(columns={'supplier_factset_entity_id':'from_id', 'customer_factset_entity_id':'to_id', \"grade\":\"weight\"})\n",
    "    current_mutual_supplier_df = current_mutual_supplier_df.groupby(['from_id','to_id'])[['weight']].mean().reset_index().fillna(0)\n",
    "    current_mutual_supplier_df = projection(current_mutual_supplier_df, sp500_entity_ids, mutual_supplier=True)\n",
    "    current_mutual_supplier_df.loc[:,'rel_type'] = 'MUTUAL_SUPPLIER'\n",
    "    current_mutual_supplier_df.loc[:,'net_id'] = d\n",
    "    edges_df = edges_df.append(current_mutual_supplier_df)\n",
    "    \n",
    "    # ------------ Mutual customer layer ------------------\n",
    "    current_mutual_customer_df = current_neighbors_supl_chain[~current_neighbors_supl_chain['customer_factset_entity_id'].isin(sp500_entity_ids)]\n",
    "    current_mutual_customer_df = current_mutual_customer_df.groupby('customer_factset_entity_id').filter(lambda x: x['supplier_factset_entity_id'].count() > 2).sort_values('customer_factset_entity_id')\n",
    "    current_mutual_customer_df = current_mutual_customer_df[['supplier_factset_entity_id','customer_factset_entity_id','grade','ranked_factset_entity_id']].rename(columns={'supplier_factset_entity_id':'from_id', 'customer_factset_entity_id':'to_id',\"grade\":\"weight\"})\n",
    "    current_mutual_customer_df = current_mutual_customer_df.groupby(['from_id','to_id'])[['weight']].mean().reset_index().fillna(0)\n",
    "    current_mutual_customer_df = projection(current_mutual_customer_df, sp500_entity_ids, mutual_supplier=False)\n",
    "    current_mutual_customer_df.loc[:,'rel_type'] = 'MUTUAL_CUSTOMER'\n",
    "    current_mutual_customer_df.loc[:,'net_id'] = d\n",
    "    edges_df = edges_df.append(current_mutual_customer_df)\n",
    "    \n",
    "\n",
    "        # Now combine the layers into a multiplex one\n",
    "    amt_nodes = {}\n",
    "    current_edges = edges_df[edges_df['net_id'] == pd.to_datetime(d)]\n",
    "    # max_grade = current_edges[current_edges['rel_type']!='OVERLAP']['weight'].max()\n",
    "\n",
    "    # within_sp100_df = current_edges[current_edges['rel_type']=='DIRECT_SUPPLIER']\n",
    "    # within_sp100_df.loc[:,'weight'] = within_sp100_df['weight']/max_grade\n",
    "    # within_sp100_df.loc[:,'rel_type'] = 'SP100'\n",
    "    # within_sp100_df.loc[:,'from_id'] += '_W'\n",
    "    # within_sp100_df.loc[:,'to_id'] += '_W'\n",
    "    # new_edges_df = new_edges_df.append(within_sp100_df[['from_id','to_id','weight','rel_type','net_id']])\n",
    "    # amt_nodes['_W'] = len(set(within_sp100_df['from_id']).union(set(within_sp100_df['to_id'])))\n",
    "\n",
    "    current_mutual_supplier_df = current_edges[current_edges['rel_type']=='MUTUAL_SUPPLIER']\n",
    "    max_weight = current_mutual_supplier_df['weight'].max()\n",
    "    current_mutual_supplier_df.loc[:,'weight'] = current_mutual_supplier_df['weight']/max_weight\n",
    "    current_mutual_supplier_df.loc[:,'from_id'] += '_S'\n",
    "    current_mutual_supplier_df.loc[:,'to_id'] += '_S'\n",
    "    new_edges_df = new_edges_df.append(current_mutual_supplier_df[['from_id','to_id','weight','rel_type','net_id']])\n",
    "    mutual_supplier_graph = build_rel_graph(current_mutual_supplier_df[['from_id','to_id','weight','rel_type','net_id']], weight='weight', directed=False)\n",
    "    mutual_supplier_pagerank = pd.DataFrame(mutual_supplier_graph.pagerank(weights='weight', directed=False), index=mutual_supplier_graph.vs['name'], columns=['pagerank_S'])\n",
    "    mutual_supplier_pagerank.loc[:,'company_id'] = mutual_supplier_pagerank.index.str[:8]\n",
    "    amt_nodes['_S'] = len(list(set(current_mutual_supplier_df['from_id']).union(set(current_mutual_supplier_df['to_id']))))\n",
    "\n",
    "    current_mutual_customer_df = current_edges[current_edges['rel_type']=='MUTUAL_CUSTOMER']\n",
    "    max_weight = current_mutual_customer_df['weight'].max()\n",
    "    current_mutual_customer_df.loc[:,'weight'] = current_mutual_customer_df['weight']/max_weight\n",
    "    current_mutual_customer_df.loc[:,'from_id'] += '_C'\n",
    "    current_mutual_customer_df.loc[:,'to_id'] += '_C'\n",
    "    new_edges_df = new_edges_df.append(current_mutual_customer_df[['from_id','to_id','weight','rel_type','net_id']])\n",
    "    mutual_customer_graph = build_rel_graph(current_mutual_customer_df[['from_id','to_id','weight','rel_type','net_id']], weight='weight', directed=False)\n",
    "    mutual_customer_pagerank = pd.DataFrame(mutual_customer_graph.pagerank(weights='weight', directed=False), index=mutual_customer_graph.vs['name'], columns=['pagerank_C'])\n",
    "    mutual_customer_pagerank.loc[:,'company_id'] = mutual_customer_pagerank.index.str[:8]\n",
    "    amt_nodes['_C'] = len(list(set(current_mutual_customer_df['from_id']).union(set(current_mutual_customer_df['to_id']))))\n",
    "\n",
    "\n",
    "    current_overlap_df = current_edges[current_edges['rel_type']=='OVERLAP']\n",
    "    max_weight = current_overlap_df['weight'].max()\n",
    "    current_overlap_df.loc[:,'weight'] = current_overlap_df['weight']/max_weight\n",
    "    current_overlap_df.loc[:,'from_id'] += '_O'\n",
    "    current_overlap_df.loc[:,'to_id'] += '_O'\n",
    "    new_edges_df = new_edges_df.append(current_overlap_df[['from_id','to_id','weight','rel_type','net_id']])\n",
    "    overlap_graph = build_rel_graph(current_overlap_df[['from_id','to_id','weight','rel_type','net_id']], weight='weight', directed=False)\n",
    "    overlap_pagerank = pd.DataFrame(overlap_graph.pagerank(weights='weight', directed=False), index=overlap_graph.vs['name'], columns=['pagerank_O'])\n",
    "    overlap_pagerank.loc[:,'company_id'] = overlap_pagerank.index.str[:8]\n",
    "    amt_nodes['_O'] = len(list(set(current_overlap_df['from_id']).union(set(current_overlap_df['to_id']))))\n",
    "\n",
    "    corr_T_edge_list['weight'] /= corr_T_edge_list['weight'].max()\n",
    "    corr_T_edge_list.loc[:,'from_id'] += '_R'\n",
    "    corr_T_edge_list.loc[:,'to_id'] += '_R'\n",
    "    new_edges_df = new_edges_df.append(corr_T_edge_list[['from_id','to_id','weight','rel_type','net_id']])\n",
    "    correlation_graph = build_rel_graph(corr_T_edge_list[['from_id','to_id','weight','rel_type','net_id']], weight='weight', directed=False)\n",
    "    \n",
    "    correlation_pagerank = pd.DataFrame(correlation_graph.pagerank(weights='weight', directed=False), index=correlation_graph.vs['name'], columns=['pagerank_R'])\n",
    "    correlation_pagerank.loc[:,'company_id'] = correlation_pagerank.index.str[:8]\n",
    "    \n",
    "    amt_nodes['_R'] = len(list(set(corr_T_edge_list['from_id']).union(set(corr_T_edge_list['to_id']))))\n",
    "\n",
    "    new_edges_df = new_edges_df[['from_id','to_id','weight','rel_type','net_id']]\n",
    "    total_nodes = sum(amt_nodes.values())\n",
    "\n",
    "    # create inter_layer edges\n",
    "\n",
    "    l = amt_nodes.keys()\n",
    "\n",
    "    current_new_edges_df = new_edges_df[new_edges_df['net_id']==d]\n",
    "    weighted_degree_df = pd.merge(current_new_edges_df.groupby('from_id')[['weight']].sum(),\n",
    "         current_new_edges_df.groupby('to_id')[['weight']].sum(),\n",
    "         how='outer',\n",
    "         left_index=True,\n",
    "         right_index=True).fillna(0).sum(axis=1)\n",
    "    current_nodes = weighted_degree_df.index.str[:-2].values\n",
    "    ed = []\n",
    "    for x in current_nodes:\n",
    "        for l1 in range(len(l)-1):\n",
    "            for l2 in range(l1+1, len(l)): \n",
    "                row = []\n",
    "                row.append(x+l[l1])\n",
    "                row.append(x+l[l2])\n",
    "                if x in sp500_entity_ids:\n",
    "                    if l[l1]=='_R' and x+l[l2] in weighted_degree_df.index:\n",
    "                        try: \n",
    "                            inter_weight = (weighted_degree_df[row[0]] + weighted_degree_df[row[1]]) / 2\n",
    "                        except KeyError:\n",
    "                            inter_weight = 0\n",
    "                    elif l[l2]=='_R' and x+l[l1] in weighted_degree_df.index:\n",
    "                        try: \n",
    "                            inter_weight = (weighted_degree_df[row[0]] + weighted_degree_df[row[1]]) / 2\n",
    "                        except KeyError:\n",
    "                            inter_weight = 0\n",
    "                    else:\n",
    "                        try: \n",
    "                            inter_weight = (weighted_degree_df[row[0]] + weighted_degree_df[row[1]]) / 2\n",
    "                        except KeyError:\n",
    "                            continue\n",
    "                else: \n",
    "                    try:\n",
    "                        inter_weight = (weighted_degree_df[row[0]] + weighted_degree_df[row[1]]) / 2\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "                row.append(inter_weight)\n",
    "                row.append('INTER')\n",
    "                row.append(d)\n",
    "                ed.append(row)\n",
    "\n",
    "    ed = pd.DataFrame(ed)\n",
    "    ed.columns = ['from_id','to_id','weight','rel_type','net_id']\n",
    "    new_edges_df = new_edges_df.append(ed)\n",
    "\n",
    "    #build network and get top 10 pagerank centrality\n",
    "\n",
    "    multiplex_supl_chain = build_rel_graph(new_edges_df[new_edges_df.net_id == d], weight='weight', directed=False)\n",
    "    # corr_missing = (ed[(ed['to_id'].str[-1]=='R') & (ed['to_id'].str[:-2].isin(sp100_tickers))].groupby('to_id')['weight'].max()==0).mean()\n",
    "    # damping = (missing_pct.loc[d].sum() + corr_missing)/6.0\n",
    "    # print(\"average missing data is \"+str(damping))\n",
    "    # print(\"corr missing data is \"+str(corr_missing))\n",
    "    # result = pd.DataFrame(multiplex_supl_chain.pagerank(weights='weight', directed=False, damping=1.0-damping), index=multiplex_supl_chain.vs['name'], columns=['pagerank'])\n",
    "    result = pd.DataFrame(multiplex_supl_chain.pagerank(weights='weight', directed=False), index=multiplex_supl_chain.vs['name'], columns=['pagerank'])\n",
    "    \n",
    "    result.loc[:,'company_id'] = result.index.str[:8]\n",
    "    result.loc[:,'rel_type'] = result.index.str[-2:]\n",
    "    result = result.pivot(columns='rel_type', index='company_id', values='pagerank')\n",
    "    weighted_degree_result = pd.DataFrame(multiplex_supl_chain.strength(weights='weight', loops=False), index=multiplex_supl_chain.vs['name'], columns=['weighted_degree'])\n",
    "    weighted_degree_result.loc[:,'company_id'] = weighted_degree_result.index.str[:8]\n",
    "    weighted_degree_result.loc[:,'rel_type'] = weighted_degree_result.index.str[-2:]\n",
    "    weighted_degree_result = weighted_degree_result.pivot(columns='rel_type', index='company_id', values='weighted_degree')\n",
    "    result = result.merge(mutual_supplier_pagerank, left_index=True, right_on='company_id', how='outer').set_index('company_id')\n",
    "    result = result.merge(mutual_customer_pagerank, left_index=True, right_on='company_id', how='outer').set_index('company_id')\n",
    "    result = result.merge(overlap_pagerank, left_index=True, right_on='company_id', how='outer').set_index('company_id')\n",
    "    result = result.merge(correlation_pagerank, left_index=True, right_on='company_id', how='outer').set_index('company_id')\n",
    "    \n",
    "    for r in l:\n",
    "        result.loc[:,'adj'+r] = result[r] * (float(amt_nodes[r]) / total_nodes)\n",
    "        weighted_degree_result.loc[:,'adj'+r] = weighted_degree_result[r] * (float(amt_nodes[r]) / total_nodes)\n",
    "    result.loc[:,'pagerank'] = result[['adj'+r for r in l]].sum(axis=1)\n",
    "    result.loc[:,'net_id'] = d\n",
    "    result = result.sort_values(['pagerank'], ascending=False)\n",
    "    weighted_degree_result.loc[:,'weighted_degree'] = weighted_degree_result[['adj'+r for r in l]].sum(axis=1)\n",
    "    weighted_degree_result.loc[:,'net_id'] = d\n",
    "    weighted_degree_result = weighted_degree_result.sort_values(['weighted_degree'], ascending=False)\n",
    "    table = table.append(result)\n",
    "    weighted_degree_table = weighted_degree_table.append(weighted_degree_result)\n",
    "    \n",
    "ids = edges_df[['from_id','to_id']].values\n",
    "ids.sort(axis=1)\n",
    "edges_df[['from_id','to_id']] = ids\n",
    "\n",
    "ids = new_edges_df[['from_id','to_id']].values\n",
    "ids.sort(axis=1)\n",
    "new_edges_df[['from_id','to_id']] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "new_edges_df.to_hdf(\"results/sp500_projection_multiplex_edges_df.h5\", key='multiplex_edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "normalized_edges_df = pd.DataFrame()\n",
    "\n",
    "for d in dates:\n",
    "    print d\n",
    "    d = pd.to_datetime(d,format=\"%Y-%m-%d\")\n",
    "    current_edges_df = edges_df[edges_df['net_id']==d]\n",
    "    \n",
    "    for l in current_edges_df['rel_type'].unique():\n",
    "        current_layer = current_edges_df[current_edges_df['rel_type'] == l]\n",
    "        current_layer.loc[:,'weight'] = current_layer['weight']/current_layer['weight'].max()\n",
    "        normalized_edges_df = normalized_edges_df.append(current_layer)\n",
    "        \n",
    "overall_normalized_edges_df = pd.DataFrame()\n",
    "for l in edges_df['rel_type'].unique():\n",
    "    current_layer = edges_df[edges_df['rel_type'] == l]\n",
    "    current_layer.loc[:,'weight'] = current_layer['weight']/current_layer['weight'].max()\n",
    "    overall_normalized_edges_df = overall_normalized_edges_df.append(current_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "import colorsys\n",
    "\n",
    "def HSVToRGB(h, s, v):\n",
    "    (r, g, b) = colorsys.hsv_to_rgb(h, s, v)\n",
    "    return (int(255*r), int(255*g), int(255*b))\n",
    " \n",
    "def getDistinctColors(n):\n",
    "    huePartition = 1.0 / (n + 1)\n",
    "    return ['#%02x%02x%02x' % HSVToRGB(huePartition * value, 0.9, 0.9) for value in range(0, n)]\n",
    "\n",
    "factset_sector_desc = pd.read_csv(\"sp100_projection_node_centralities.csv\")['factset_sector_desc'].unique()\n",
    "\n",
    "# we need r color names. \n",
    "color_names = ['red','orangered','darkgoldenrod1','yellow2','chartreuse','green','springgreen2','mediumspringgreen','cyan','deepskyblue2','blue','deepskyblue2','purple','magenta2','deeppink','firebrick2']\n",
    "# color_dict = {sorted(factset_sector_desc)[i]:getDistinctColors(len(factset_sector_desc))[i]\n",
    "#               for i in range(len(factset_sector_desc))}\n",
    "color_dict = {sorted(factset_sector_desc)[i]:color_names[i]\n",
    "               for i in range(len(factset_sector_desc))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "weight_distribution = {}\n",
    "degree_distribution = {}\n",
    "degree_df = {}\n",
    "sector_degree = {}\n",
    "distinct_nodes = node_df.drop_duplicates()\n",
    "for r in overall_normalized_edges_df['rel_type'].unique():\n",
    "    dist = overall_normalized_edges_df[overall_normalized_edges_df['rel_type']==r].groupby('net_id').mean().rename(columns={'weight':'mean'})\n",
    "    dist['std'] = overall_normalized_edges_df[overall_normalized_edges_df['rel_type']==r].groupby('net_id').std()\n",
    "    dist=dist.reset_index()\n",
    "    dist['net_id'] = pd.to_datetime(dist['net_id'], format=\"%Y-%m-%d\")\n",
    "    weight_distribution[r] = dist\n",
    "    layer_df = overall_normalized_edges_df[overall_normalized_edges_df['rel_type']==r]\n",
    "    degree_by_layer = layer_df.groupby(['net_id','from_id']).sum().reset_index().pivot(columns='net_id', index='from_id', values='weight').reindex(set(layer_df['from_id']).union(set(layer_df['to_id']))).fillna(0)\n",
    "    degree_by_layer +=layer_df.groupby(['net_id','to_id']).sum().reset_index().pivot(columns='net_id', index='to_id', values='weight').reindex(set(layer_df['from_id']).union(set(layer_df['to_id']))).fillna(0)\n",
    "    degree_by_layer = degree_by_layer.replace(0.0, np.nan)\n",
    "    degree_by_layer.index.name = \"node_id\"\n",
    "    \n",
    "    degree_df[r] = degree_by_layer\n",
    "    degree_distribution[r] = pd.DataFrame(degree_by_layer.mean(), columns=['mean'])\n",
    "    degree_distribution[r]['std'] = degree_by_layer.std()\n",
    "    sector_degree[r] = pd.merge(degree_by_layer.reset_index(), distinct_nodes, left_on=['node_id'], right_on=['vertex_id'], how='left').drop('node_id', axis=1).groupby('factset_sector_desc').mean().drop(['sector_code','primary_sic_code','industry_code'], axis=1).reset_index()\n",
    "    \n",
    "degree_df['COMPETITION'] = degree_df['OVERLAP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "exp_name = \"sp500\"\n",
    "new_edges_df= pd.read_hdf(\"results/sp500_projection_multiplex_edges_df.h5\", key='multiplex_edges')\n",
    "d = pd.to_datetime('2003-07-01',format=\"%Y-%m-%d\")\n",
    "current_edge_list = new_edges_df[new_edges_df['net_id']==d].replace('OVERLAP','COMPETITION')\n",
    "current_edge_list['from_id_short'] = current_edge_list['from_id'].str[:-2]\n",
    "current_edge_list['to_id_short'] = current_edge_list['to_id'].str[:-2]\n",
    "\n",
    "current_edge_list = pd.merge(current_edge_list, node_df[['vertex_id','ticker','factset_sector_desc']], left_on='from_id_short', right_on='vertex_id', how='left').drop('vertex_id', axis=1).rename(columns={'ticker':'from_ticker','factset_sector_desc':'from_sector'})\n",
    "current_edge_list = pd.merge(current_edge_list, node_df[['vertex_id','ticker','factset_sector_desc']], left_on='to_id_short', right_on='vertex_id', how='left').drop('vertex_id', axis=1).rename(columns={'ticker':'to_ticker','factset_sector_desc':'to_sector'})\n",
    "current_edge_list['from_layer'] = current_edge_list['rel_type']\n",
    "current_edge_list['to_layer'] = current_edge_list['rel_type']\n",
    "current_inter = current_edge_list[current_edge_list['rel_type']=='INTER']\n",
    "current_edge_list = current_edge_list[current_edge_list['rel_type']!='INTER']\n",
    "layer_dict = {\"_O\":\"COMPETITION\", \"_S\":\"MUTUAL_SUPPLIER\", \"_R\":\"CORRELATION\", \"_C\":\"MUTUAL_CUSTOMER\"}\n",
    "for r in layer_dict.keys():\n",
    "    current_inter.loc[current_inter['from_id'].str[-2:]==r,'from_layer']=layer_dict[r]\n",
    "    current_inter.loc[current_inter['to_id'].str[-2:]==r,'to_layer']=layer_dict[r]\n",
    "current_edge_list = current_edge_list.append(current_inter)\n",
    "node_info = pd.DataFrame(list(set(current_edge_list['from_ticker']).union(set(current_edge_list['to_ticker']))))\n",
    "node_info.index+=1\n",
    "node_info = node_info.reset_index()\n",
    "node_info.columns = ['nodeID','nodeLabel']\n",
    "layer_info = pd.DataFrame(list(set(current_edge_list['from_layer']).union(set(current_edge_list['to_layer']))))\n",
    "layer_info.index+=1\n",
    "layer_info = layer_info.reset_index()\n",
    "layer_info.columns = ['layerID','layerLabel']\n",
    "\n",
    "current_edge_list = current_edge_list.merge(node_info, left_on='from_ticker', right_on='nodeLabel').drop('nodeLabel',axis=1).rename(columns={'nodeID':'from_nodeID'})\n",
    "current_edge_list = current_edge_list.merge(node_info, left_on='to_ticker', right_on='nodeLabel').drop('nodeLabel',axis=1).rename(columns={'nodeID':'to_nodeID'})\n",
    "current_edge_list = current_edge_list.merge(layer_info, left_on='from_layer', right_on='layerLabel').drop('layerLabel',axis=1).rename(columns={'layerID':'from_layerID'})\n",
    "current_edge_list = current_edge_list.merge(layer_info, left_on='to_layer', right_on='layerLabel').drop('layerLabel',axis=1).rename(columns={'layerID':'to_layerID'})\n",
    "\n",
    "edge_list = current_edge_list[['from_nodeID', 'from_layerID','to_nodeID','to_layerID','weight']]\n",
    "\n",
    "edge_list.to_csv('results/muxviz/'+exp_name+d.strftime(\"%Y-%m-%d\")+\"_edge_list.csv\", index=False, header=False, sep=' ')\n",
    "node_info.to_csv('results/muxviz/'+exp_name+d.strftime(\"%Y-%m-%d\")+\"_node_info.csv\", index=False, header=True, sep=' ')\n",
    "layer_info.to_csv('results/muxviz/'+exp_name+d.strftime(\"%Y-%m-%d\")+\"_layer_info.csv\", index=False, header=True, sep=' ')\n",
    "with open('/home/xx2167/networkClustering/results/muxviz/'+d.strftime(\"%Y-%m-%d\")+\"_config.txt\", \"w\") as text_file:\n",
    "    text_file.write('/home/xx2167/networkClustering/results/muxviz/'+d.strftime(\"%Y-%m-%d\")+\"_edge_list.csv\"+\";\"+'/home/xx2167/networkClustering/results/muxviz/'+d.strftime(\"%Y-%m-%d\")+\"_layer_info.csv\"+\";\"+'/home/xx2167/networkClustering/results/muxviz/'+d.strftime(\"%Y-%m-%d\")+\"_node_info.csv\")\n",
    "    \n",
    "node_size_color = current_edge_list[['from_id_short','from_ticker','from_layer', 'from_sector']].rename(columns={'from_ticker':'nodeLabel', 'from_layer':'layerLabel', 'from_sector':'sector','from_id_short':'id'}).append(\n",
    "    current_edge_list[['to_id_short','to_ticker','to_layer','to_sector']].rename(columns={'to_ticker':'nodeLabel', 'to_layer':'layerLabel','to_sector':'sector','to_id_short':'id'})).drop_duplicates().reset_index(drop=True)\n",
    "node_size_color = pd.merge(node_size_color, node_info).merge(layer_info)\n",
    "node_size_color['color'] = node_size_color['sector'].map(color_dict)\n",
    "for r in node_size_color['layerLabel'].unique():\n",
    "    temp_degree_df = degree_df[r][[d]]\n",
    "    node_size_color.loc[node_size_color['layerLabel']==r, 'size'] = pd.merge(node_size_color[node_size_color['layerLabel']==r], temp_degree_df, left_on='id', right_index=True, how='left')[d]\n",
    "node_size_color[['nodeID','layerID','color','size']].to_csv('results/muxviz/'+exp_name+d.strftime(\"%Y-%m-%d\")+\"_node_color_size.csv\", index=False, header=True, sep=' ')\n",
    "\n",
    "node_size_color[['nodeLabel','layerLabel','color','size']].to_csv('results/muxviz/'+d.strftime(\"%Y-%m-%d\")+\"_node_color_size.csv\", index=False, header=True, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "for r in edges_df['rel_type'].unique():\n",
    "    weight_distribution[r].to_hdf(\"results/centrality/sp500_weight_distribution.h5\", key=r)\n",
    "    degree_distribution[r].to_hdf(\"results/centrality/sp500_degree_distribution.h5\", key=r)\n",
    "    degree_df[r].to_hdf(\"results/centrality/sp500_degree_df.h5\", key=r)\n",
    "    sector_degree[r].to_hdf(\"results/centrality/sp500_sector_degree.h5\", key=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "weighted_degree = {}\n",
    "for r in overall_normalized_edges_df['rel_type'].unique():\n",
    "    layer_top_10 = degree_df[r].reset_index().melt(id_vars='node_id',value_vars=list(degree_df[r].columns),value_name='weight').groupby(['net_id','node_id']).sum().reset_index().groupby('net_id').apply(lambda x: x.sort_values('weight', ascending=False).head(10)).reset_index(drop=True)\n",
    "    weighted_degree[r] = pd.merge(layer_top_10, node_df, left_on=['node_id'], right_on=['vertex_id'], how='left').drop('node_id', axis=1)\n",
    "    weighted_degree[r].to_hdf(\"results/centrality/sp500_weighted_degree_centralities_top10.h5\", key=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "weighted_degree = {}\n",
    "for r in overall_normalized_edges_df['rel_type'].unique():\n",
    "    layer_top_50 = degree_df[r].reset_index().melt(id_vars='node_id',value_vars=list(degree_df[r].columns),value_name='weight').groupby(['net_id','node_id']).sum().reset_index().groupby('net_id').apply(lambda x: x.sort_values('weight', ascending=False).head(50)).reset_index(drop=True)\n",
    "    weighted_degree[r] = pd.merge(layer_top_50, node_df, left_on=['node_id'], right_on=['vertex_id'], how='left').drop('node_id', axis=1)\n",
    "    weighted_degree[r].to_hdf(\"results/centrality/sp500_weighted_degree_centralities_top50.h5\", key=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "edges_df.to_hdf(\"results/centrality/sp500_edges_df.h5\", key=\"edges_df\")\n",
    "normalized_edges_df.to_hdf(\"results/centrality/sp500_edges_df.h5\", key=\"normalized_edges_df\")\n",
    "overall_normalized_edges_df.to_hdf(\"results/centrality/sp500_edges_df.h5\", key=\"overall_normalized_edges_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "node_centrality_df = pd.merge(table.reset_index()[['company_id','net_id','pagerank']], node_df, left_on=['company_id'], right_on=['vertex_id'], how='left').drop('company_id', axis=1)\n",
    "table.to_csv('sp500__projection_centralities.csv')\n",
    "node_centrality_df.to_csv('sp500_projection_node_centralities.csv', index=False, encoding='utf-8')\n",
    "\n",
    "node_weighted_degree_centrality_df = pd.merge(weighted_degree_table.reset_index()[['company_id','net_id','weighted_degree']], node_df, left_on=['company_id'], right_on=['vertex_id'], how='left').drop('company_id', axis=1)\n",
    "weighted_degree_table.to_csv('/mnt/hdd/results/centrality/sp500_projection_weighted_degree_centralities.csv')\n",
    "node_weighted_degree_centrality_df.to_csv('/mnt/hdd/results/centrality/sp500_projection_node_weighted_degree_centralities.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "node_centrality_df_S = pd.merge(table.reset_index()[['company_id','net_id','pagerank_S']], node_df, left_on=['company_id'], right_on=['vertex_id'], how='left').drop('company_id', axis=1)\n",
    "node_centrality_df_C = pd.merge(table.reset_index()[['company_id','net_id','pagerank_C']], node_df, left_on=['company_id'], right_on=['vertex_id'], how='left').drop('company_id', axis=1)\n",
    "node_centrality_df_O = pd.merge(table.reset_index()[['company_id','net_id','pagerank_O']], node_df, left_on=['company_id'], right_on=['vertex_id'], how='left').drop('company_id', axis=1)\n",
    "node_centrality_df_R = pd.merge(table.reset_index()[['company_id','net_id','pagerank_R']], node_df, left_on=['company_id'], right_on=['vertex_id'], how='left').drop('company_id', axis=1)\n",
    "node_centrality_df_S.groupby('net_id').apply(lambda x: x.sort_values('pagerank_S', ascending=False).head(10)).to_csv('sp500_projection_node_centralities_S_top10.csv', index=False, encoding='utf-8')\n",
    "node_centrality_df_C.groupby('net_id').apply(lambda x: x.sort_values('pagerank_C', ascending=False).head(10)).to_csv('sp500_projection_node_centralities_C_top10.csv', index=False, encoding='utf-8')\n",
    "node_centrality_df_O.groupby('net_id').apply(lambda x: x.sort_values('pagerank_O', ascending=False).head(10)).to_csv('sp500_projection_node_centralities_O_top10.csv', index=False, encoding='utf-8')\n",
    "node_centrality_df_R.groupby('net_id').apply(lambda x: x.sort_values('pagerank_R', ascending=False).head(10)).to_csv('sp500_projection_node_centralities_R_top10.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "node_centrality_df_S = pd.merge(table.reset_index()[['company_id','net_id','pagerank_S']], node_df, left_on=['company_id'], right_on=['vertex_id'], how='left').drop('company_id', axis=1)\n",
    "node_centrality_df_C = pd.merge(table.reset_index()[['company_id','net_id','pagerank_C']], node_df, left_on=['company_id'], right_on=['vertex_id'], how='left').drop('company_id', axis=1)\n",
    "node_centrality_df_O = pd.merge(table.reset_index()[['company_id','net_id','pagerank_O']], node_df, left_on=['company_id'], right_on=['vertex_id'], how='left').drop('company_id', axis=1)\n",
    "node_centrality_df_R = pd.merge(table.reset_index()[['company_id','net_id','pagerank_R']], node_df, left_on=['company_id'], right_on=['vertex_id'], how='left').drop('company_id', axis=1)\n",
    "node_centrality_df_S.groupby('net_id').apply(lambda x: x.sort_values('pagerank_S', ascending=False).head(50)).to_csv('sp500_projection_node_centralities_S_top50.csv', index=False, encoding='utf-8')\n",
    "node_centrality_df_C.groupby('net_id').apply(lambda x: x.sort_values('pagerank_C', ascending=False).head(50)).to_csv('sp500_projection_node_centralities_C_top50.csv', index=False, encoding='utf-8')\n",
    "node_centrality_df_O.groupby('net_id').apply(lambda x: x.sort_values('pagerank_O', ascending=False).head(50)).to_csv('sp500_projection_node_centralities_O_top50.csv', index=False, encoding='utf-8')\n",
    "node_centrality_df_R.groupby('net_id').apply(lambda x: x.sort_values('pagerank_R', ascending=False).head(50)).to_csv('sp500_projection_node_centralities_R_top50.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "node_weighted_degree_centrality_df.groupby('net_id').head(10).to_hdf(\"results/centrality/sp500_weighted_degree_centralities_top10.h5\", key='multiplex')\n",
    "node_weighted_degree_centrality_df.groupby('net_id').head(50).to_hdf(\"results/centrality/sp500_weighted_degree_centralities_top50.h5\", key='multiplex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "node_centrality_df.groupby('net_id').head(10).to_csv('sp500_projection_node_centralities_top10.csv', index=False, encoding='utf-8')\n",
    "node_centrality_df.groupby('net_id').head(50).to_csv('sp500_projection_node_centralities_top50.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "edges_df = pd.read_hdf(\"results/centrality/sp500_edges_df.h5\", key=\"edges_df\")\n",
    "normalized_edges_df= pd.read_hdf(\"results/centrality/sp500_edges_df.h5\", key=\"normalized_edges_df\")\n",
    "overall_normalized_edges_df = pd.read_hdf(\"results/centrality/sp500_edges_df.h5\", key=\"overall_normalized_edges_df\")\n",
    "weight_distribution = {}\n",
    "degree_distribution = {}\n",
    "degree_df = {}\n",
    "sector_degree = {}\n",
    "for r in edges_df['rel_type'].unique():\n",
    "    weight_distribution[r]=pd.read_hdf(\"results/centrality/sp500_weight_distribution.h5\", key=r)\n",
    "    degree_distribution[r]=pd.read_hdf(\"results/centrality/sp500_degree_distribution.h5\", key=r)\n",
    "    degree_df[r]=pd.read_hdf(\"results/centrality/sp500_degree_df.h5\", key=r)\n",
    "    sector_degree[r]=pd.read_hdf(\"results/centrality/sp500_sector_degree.h5\", key=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "for r in edges_df['rel_type'].unique():\n",
    "    weight_distribution[r].to_csv(\"results/centrality/sp500_weight_distribution_\"+r+\".csv\", index=False)\n",
    "    degree_distribution[r].to_csv(\"results/centrality/sp500_degree_distribution_\"+r+\".csv\")\n",
    "    degree_df[r].to_csv(\"results/centrality/sp500_degree_df_\"+r+\".csv\")\n",
    "    sector_degree[r].to_csv(\"results/centrality/sp500_sector_degree_\"+r+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "# result: node degrees by layer, overlap degree, and participation coeff\n",
    "result = pd.DataFrame()\n",
    "# layer_corr_df: layer degree correlations\n",
    "layer_corr_df = pd.DataFrame()\n",
    "for d in dates:\n",
    "    d = d[-10:]\n",
    "    print(d)\n",
    "    degree_table = pd.DataFrame.from_dict({\"mutual_customer_degree\":degree_df['MUTUAL_CUSTOMER'][d],\"mutual_supplier_degree\":degree_df['MUTUAL_SUPPLIER'][d],\"comp_degree\":degree_df['OVERLAP'][d],\"corr_degree\":degree_df['CORRELATION'][d]})\n",
    "    degree_table['overlap'] = degree_table['mutual_customer_degree'] + degree_table['mutual_supplier_degree'] + degree_table['comp_degree'] + degree_table['corr_degree']\n",
    "    degree_table['participation_coef'] = (1-(degree_table['mutual_customer_degree']**2+degree_table['mutual_supplier_degree']**2+degree_table['comp_degree']**2+degree_table['corr_degree']**2    )/degree_table['overlap']**2)*4/2\n",
    "    degree_table = degree_table.dropna(how='all')\n",
    "    degree_table = degree_table.merge(sp500_entity_id, left_index=True, right_on='factset_entity_id')\n",
    "    degree_table['net_id'] = pd.to_datetime(d, format=\"%Y-%m-%d\")\n",
    "    result = result.append(degree_table[['factset_entity_id', 'ticker', 'mutual_customer_degree','mutual_supplier_degree', 'corr_degree', 'comp_degree', 'overlap', 'participation_coef', 'net_id']])\n",
    "    corr_table = {}\n",
    "    nodes = degree_table[degree_table['ticker'] != 0]\n",
    "    corr_table['mutual_customer_&_corr'] = nodes[['mutual_customer_degree', 'corr_degree']].fillna(0).corr().iat[0,1]\n",
    "    corr_table['mutual_supplier_&_corr'] = nodes[['mutual_supplier_degree', 'corr_degree']].fillna(0).corr().iat[0,1]\n",
    "    corr_table['comp_&_corr'] = nodes[['corr_degree', 'comp_degree']].fillna(0).corr().iat[0,1]\n",
    "    corr_table['mutual_customer_&_comp'] = nodes[['mutual_customer_degree', 'comp_degree']].fillna(0).corr().iat[0,1]\n",
    "    corr_table['mutual_supplier_&_comp'] = nodes[['mutual_supplier_degree', 'comp_degree']].fillna(0).corr().iat[0,1]\n",
    "    corr_table['mutual_customer_&_mutual_supplier'] = nodes[['mutual_supplier_degree', 'mutual_customer_degree']].fillna(0).corr().iat[0,1]\n",
    "    corr_table['net_id'] = d\n",
    "    corr_df = pd.DataFrame(corr_table, index = [0])\n",
    "    layer_corr_df = layer_corr_df.append(corr_df[['comp_&_corr', 'mutual_customer_&_corr', 'mutual_supplier_&_corr','mutual_customer_&_comp', 'mutual_supplier_&_comp','mutual_customer_&_mutual_supplier', 'net_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "# result: node degrees by layer, overlap degree, and participation coeff\n",
    "result = pd.DataFrame()\n",
    "# layer_corr_df: layer degree correlations\n",
    "layer_corr_df = pd.DataFrame()\n",
    "for d in dates:\n",
    "    d = d[-10:]\n",
    "    print(d)\n",
    "    degree_table = pd.DataFrame.from_dict({\"mutual_customer_degree\":degree_df['MUTUAL_CUSTOMER'][d],\"mutual_supplier_degree\":degree_df['MUTUAL_SUPPLIER'][d],\"comp_degree\":degree_df['OVERLAP'][d],\"corr_degree\":degree_df['CORRELATION'][d]})\n",
    "    degree_table['overlap'] = degree_table['mutual_customer_degree'] + degree_table['mutual_supplier_degree'] + degree_table['comp_degree'] + degree_table['corr_degree']\n",
    "    degree_table['participation_coef'] = (1-(degree_table['mutual_customer_degree']**2+degree_table['mutual_supplier_degree']**2+degree_table['comp_degree']**2+degree_table['corr_degree']**2    )/degree_table['overlap']**2)*4/2\n",
    "    degree_table = degree_table.dropna(how='all')\n",
    "    degree_table = degree_table.merge(sp500_entity_id, left_index=True, right_on='factset_entity_id')\n",
    "    degree_table['net_id'] = pd.to_datetime(d, format=\"%Y-%m-%d\")\n",
    "    result = result.append(degree_table[['factset_entity_id', 'ticker', 'mutual_customer_degree','mutual_supplier_degree', 'corr_degree', 'comp_degree', 'overlap', 'participation_coef', 'net_id']])\n",
    "    corr_table = {}\n",
    "    nodes = degree_table[degree_table['ticker'] != 0]\n",
    "    corr_table['mutual_customer_&_corr'] = nodes[['mutual_customer_degree', 'corr_degree']].fillna(0).corr().iat[0,1]\n",
    "    corr_table['mutual_supplier_&_corr'] = nodes[['mutual_supplier_degree', 'corr_degree']].fillna(0).corr().iat[0,1]\n",
    "    corr_table['comp_&_corr'] = nodes[['corr_degree', 'comp_degree']].fillna(0).corr().iat[0,1]\n",
    "    corr_table['mutual_customer_&_comp'] = nodes[['mutual_customer_degree', 'comp_degree']].fillna(0).corr().iat[0,1]\n",
    "    corr_table['mutual_supplier_&_comp'] = nodes[['mutual_supplier_degree', 'comp_degree']].fillna(0).corr().iat[0,1]\n",
    "    corr_table['mutual_customer_&_mutual_supplier'] = nodes[['mutual_supplier_degree', 'mutual_customer_degree']].fillna(0).corr().iat[0,1]\n",
    "    corr_table['net_id'] = d\n",
    "    corr_df = pd.DataFrame(corr_table, index = [0])\n",
    "    layer_corr_df = layer_corr_df.append(corr_df[['comp_&_corr', 'mutual_customer_&_corr', 'mutual_supplier_&_corr','mutual_customer_&_comp', 'mutual_supplier_&_comp','mutual_customer_&_mutual_supplier', 'net_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "layer_edge_overlap = overall_normalized_edges_df.groupby(['from_id','to_id','net_id']).sum().reset_index().groupby('net_id').mean()\n",
    "\n",
    "overlapping_degree = overall_normalized_edges_df.groupby(['net_id','from_id']).sum().reset_index().pivot(columns='net_id', index='from_id', values='weight').reindex(set(overall_normalized_edges_df['from_id']).union(set(overall_normalized_edges_df['to_id']))).fillna(0)\n",
    "overlapping_degree +=overall_normalized_edges_df.groupby(['net_id','to_id']).sum().reset_index().pivot(columns='net_id', index='to_id', values='weight').reindex(set(overall_normalized_edges_df['from_id']).union(set(overall_normalized_edges_df['to_id']))).fillna(0)\n",
    "overlapping_degree = overlapping_degree.replace(0.0, np.nan)\n",
    "overlapping_degree.index.name = \"node_id\"\n",
    "\n",
    "unique_edge_fraction = (overall_normalized_edges_df.groupby(['net_id','from_id','to_id']).count()==1)*overall_normalized_edges_df.groupby(['net_id','from_id','to_id']).head().set_index(['net_id','from_id','to_id'])\n",
    "unique_edge_fraction = unique_edge_fraction.reset_index().groupby(['rel_type','net_id']).sum().drop('', axis=0)/overall_normalized_edges_df.groupby(['rel_type','net_id']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "result_file = \"/mnt/hdd/results/centrality/sp500_projection_results.h5\"\n",
    "\n",
    "node_df.to_hdf(result_file, key='nodes_df')\n",
    "new_edges_df.to_hdf(result_file, key='multiplex_edges')\n",
    "normalized_edges_df.to_hdf(result_file, key='normalized_edges')\n",
    "overall_normalized_edges_df.to_hdf(result_file, key='overall_normalized_edges')\n",
    "\n",
    "edges_df.to_hdf(result_file, key=\"edges_df\")\n",
    "\n",
    "for r in edges_df['rel_type'].unique():\n",
    "    weighted_degree[r].to_hdf(result_file, key='node_centrality_weighted_degree_top10_'+r)\n",
    "    weight_distribution[r].to_hdf(result_file, key='edge_weight_distribution_'+r)\n",
    "    degree_distribution[r].to_hdf(result_file, key='node_degree_distribution_'+r)\n",
    "    degree_df[r].to_hdf(result_file, key='node_degrees_by_layer_'+r)\n",
    "    sector_degree[r].to_hdf(result_file, key='sector_average_degree_by_layer_'+r)\n",
    "    \n",
    "table.to_hdf(result_file, key=\"pagerank_centrality_table\")\n",
    "weighted_degree_table.to_hdf(result_file, key=\"weighted_degree_centrality_table\")\n",
    "node_centrality_df.to_hdf(result_file, key=\"node_centrality_pagerank\")\n",
    "node_weighted_degree_centrality_df.to_hdf(result_file, key=\"node_centrality_weighted_degree\")\n",
    "node_centrality_df_S.groupby('net_id').apply(lambda x: x.sort_values('pagerank_S', ascending=False).head(10)).to_hdf(result_file, key=\"node_centrality_pagerank_top10_MUTUAL_SUPPLIER\")\n",
    "node_centrality_df_C.groupby('net_id').apply(lambda x: x.sort_values('pagerank_C', ascending=False).head(10)).to_hdf(result_file, key=\"node_centrality_pagerank_top10_MUTUAL_CUSTOMER\")\n",
    "node_centrality_df_O.groupby('net_id').apply(lambda x: x.sort_values('pagerank_O', ascending=False).head(10)).to_hdf(result_file, key=\"node_centrality_pagerank_top10_OVERLAP\")\n",
    "node_centrality_df_R.groupby('net_id').apply(lambda x: x.sort_values('pagerank_R', ascending=False).head(10)).to_hdf(result_file, key=\"node_centrality_pagerank_top10_CORRELATION\")\n",
    "\n",
    "layer_corr_df.to_hdf(result_file, key=\"layer_weighted_degree_correlation\")\n",
    "result.to_hdf(result_file, key=\"overlap_degree_paritipation_coeff\")\n",
    "\n",
    "layer_edge_overlap.to_hdf(result_file, key=\"layer_edge_overlap\")\n",
    "overlapping_degree.to_hdf(result_file, key=\"overlapping_degree\")\n",
    "unique_edge_fraction.to_hdf(result_file, key=\"unique_edge_fraction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "layer_edge_overlap.to_csv('results/centrality/sp500_layer_edge_overlap.csv')\n",
    "overlapping_degree.to_csv('results/centrality/sp500_overlapping_degree.csv')\n",
    "unique_edge_fraction.to_csv('results/centrality/sp500_unique_edge_fraction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
